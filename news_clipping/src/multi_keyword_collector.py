import feedparser
import json
from datetime import datetime
from pathlib import Path
import re

class MultiKeywordCollector:
    def __init__(self):
        # RSS í”¼ë“œ URL ëª©ë¡
        self.rss_feeds = {
            "ì—°í•©ë‰´ìŠ¤_ê²½ì œ": "https://www.yonhapnewstv.co.kr/category/news/economy/feed/",
            "í•œêµ­ê²½ì œ_ì‚°ì—…": "https://rss.hankyung.com/feed/economy.xml",
            "ì´íˆ¬ë‰´ìŠ¤_ì—ë„ˆì§€": "http://www.e2news.com/rss/allArticle.xml"
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ í•„í„°
        self.categories = {
            "ì—ë„ˆì§€": ["ì‹ ì¬ìƒì—ë„ˆì§€", "íƒœì–‘ê´‘", "í’ë ¥", "ESS"],
            "ë°©ì‚°": ["ë°©ìœ„ì‚°ì—…", "K-ë°©ì‚°", "í•œí™”ì‹œìŠ¤í…œ", "ë°©ì‚°ìˆ˜ì¶œ"],
            "ë³´í—˜": ["ë³´í—˜ì¤‘ê°œ", "ë³´í—˜ìƒí’ˆ", "ì¬ë³´í—˜"]
        }
        
        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        self.output_dir = Path(__file__).parent.parent / 'output'
        self.output_dir.mkdir(exist_ok=True)

    def fetch_rss_feed(self, feed_url):
        """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            feed = feedparser.parse(feed_url)
            return feed.entries
        except Exception as e:
            print(f"âŒ RSS í”¼ë“œ ìˆ˜ì§‘ ì˜¤ë¥˜ ({feed_url}): {str(e)}")
            return []

    def matches_category(self, text):
        """í…ìŠ¤íŠ¸ê°€ ì–´ë–¤ ì¹´í…Œê³ ë¦¬ì˜ í‚¤ì›Œë“œì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        for category, keywords in self.categories.items():
            for keyword in keywords:
                if keyword in text:
                    return True, category
        return False, None

    def clean_html(self, text):
        """HTML íƒœê·¸ë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
        clean = re.compile('<.*?>')
        return re.sub(clean, '', text)

    def format_date(self, date_str):
        """ë‚ ì§œ í˜•ì‹ì„ ê°„ë‹¨í•˜ê²Œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            date_obj = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S %z")
            return date_obj.strftime("%Y-%m-%d")
        except:
            return date_str

    def collect_news(self):
        """ëª¨ë“  RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
        categorized_news = {category: [] for category in self.categories.keys()}
        
        print("ğŸ” ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        
        for source, feed_url in self.rss_feeds.items():
            print(f"\nğŸ“° {source} í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
            entries = self.fetch_rss_feed(feed_url)
            
            for entry in entries:
                title = self.clean_html(entry.title)
                matches, category = self.matches_category(title)
                
                if matches:
                    news_item = {
                        "title": title,
                        "date": self.format_date(entry.get('published', '')),
                        "link": entry.link,
                        "source": source
                    }
                    categorized_news[category].append(news_item)
        
        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        output_file = self.output_dir / "multi_news_results.txt"
        total_count = 0
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for category, news_list in categorized_news.items():
                count = len(news_list)
                total_count += count
                
                f.write(f"\n=== {category} ë¶„ì•¼ ({count}ê±´) ===\n")
                print(f"\n=== {category} ë¶„ì•¼ ({count}ê±´) ===")
                
                for news in news_list:
                    line = f"- {news['title']} ({news['date']})\n"
                    f.write(line)
                    print(line.strip())
            
            f.write(f"\nì´ ìˆ˜ì§‘: {total_count}ê±´\n")
            print(f"\nâœ¨ ì´ ìˆ˜ì§‘: {total_count}ê±´")
            print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_file}")

if __name__ == "__main__":
    collector = MultiKeywordCollector()
    collector.collect_news() 